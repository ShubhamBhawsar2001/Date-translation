{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ozI5Gw8-ce8d"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Model, Input, Sequential\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Layer, Add, GRU, \\\n",
        "                                    Activation, Softmax, Concatenate, TimeDistributed\n",
        "\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import unicodedata\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Wq5IoPkwGVqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c090222a-d3b1-456c-f383-5e491cd28455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import random\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "bGFQf4NwGX2s"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the Seq2Seq model with attention\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim, output_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hidden_dim)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: (src_len, batch_size)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded: (src_len, batch_size, emb_dim)\n",
        "\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        # outputs: (src_len, batch_size, hidden_dim)\n",
        "        # hidden: (1, batch_size, hidden_dim)\n",
        "\n",
        "        # Calculate attention weights\n",
        "        attn_weights = self.attention(torch.cat((hidden.repeat(src.shape[0], 1, 1), outputs), dim=2))\n",
        "        attn_weights = torch.softmax(attn_weights, dim=0)\n",
        "\n",
        "        # Apply attention to the outputs\n",
        "        context = torch.sum(attn_weights * outputs, dim=0)\n",
        "        context = context.unsqueeze(0)\n",
        "\n",
        "        # context: (1, batch_size, hidden_dim)\n",
        "\n",
        "        prediction = self.fc_out(context)\n",
        "        # prediction: (1, batch_size, output_dim)\n",
        "\n",
        "        return prediction.squeeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FvWNL_YZRjFw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fxED780dRjC4"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"/content/drive/MyDrive/iisc/nlp/Assignment2aDataset (1).txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-eZ0NCS8SSTG"
      },
      "outputs": [],
      "source": [
        "df.columns=['input','output']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "DYdkFxkmShdQ",
        "outputId": "4afc2a89-1606-498f-af40-10b2502ca1a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        input         output\n",
              "0                '9 may 1630'   '1630-05-09'\n",
              "1                '15/03/2014'   '2014-03-15'\n",
              "2               'mar 16 1675'   '1675-03-16'\n",
              "3               'jun 16 1640'   '1640-06-16'\n",
              "4          'friday 1791 2 09'   '1791-09-02'\n",
              "...                       ...            ...\n",
              "39994      'december 26 1900'   '1900-12-26'\n",
              "39995           '15 may 1828'   '1828-05-15'\n",
              "39996  'friday april 18 1851'   '1851-04-18'\n",
              "39997          'june 11 2070'   '2070-06-11'\n",
              "39998       'january 27 1712'   '1712-01-27'\n",
              "\n",
              "[39999 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b6583b5b-56b0-425a-834d-7667dbb72af2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>'9 may 1630'</td>\n",
              "      <td>'1630-05-09'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>'15/03/2014'</td>\n",
              "      <td>'2014-03-15'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'mar 16 1675'</td>\n",
              "      <td>'1675-03-16'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>'jun 16 1640'</td>\n",
              "      <td>'1640-06-16'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>'friday 1791 2 09'</td>\n",
              "      <td>'1791-09-02'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39994</th>\n",
              "      <td>'december 26 1900'</td>\n",
              "      <td>'1900-12-26'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39995</th>\n",
              "      <td>'15 may 1828'</td>\n",
              "      <td>'1828-05-15'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39996</th>\n",
              "      <td>'friday april 18 1851'</td>\n",
              "      <td>'1851-04-18'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39997</th>\n",
              "      <td>'june 11 2070'</td>\n",
              "      <td>'2070-06-11'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39998</th>\n",
              "      <td>'january 27 1712'</td>\n",
              "      <td>'1712-01-27'</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>39999 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b6583b5b-56b0-425a-834d-7667dbb72af2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b6583b5b-56b0-425a-834d-7667dbb72af2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b6583b5b-56b0-425a-834d-7667dbb72af2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2cfbfd53-435f-4701-bea5-30b78768c394\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2cfbfd53-435f-4701-bea5-30b78768c394')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2cfbfd53-435f-4701-bea5-30b78768c394 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "ZTYEFHjdjn2o"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(df):\n",
        "    # Initialize an empty list to store tokenized sentences\n",
        "\n",
        "    df= df.str.replace('\\'',' ').astype(str)\n",
        "    df = df.str.replace('/',' ').astype(str)\n",
        "    df = df.str.replace('-',' ').astype(str)\n",
        "    tokenized_sentences = []\n",
        "\n",
        "    # Tokenize and add start and end tokens to each sentence\n",
        "    for sentence in df:\n",
        "        tokens = word_tokenize(sentence)\n",
        "        tokens = ['start'] + tokens + ['end']\n",
        "        tokenized_sentences.append(tokens)\n",
        "\n",
        "\n",
        "\n",
        "    return tokenized_sentences\n"
      ],
      "metadata": {
        "id": "_MugdUtsi7ZN"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "n9zXXPF5Wlqx"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_df=preprocessing(df['input'])\n",
        "out_df=preprocessing(df['output'])"
      ],
      "metadata": {
        "id": "j847JmpNjErh"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(in_df,out_df)"
      ],
      "metadata": {
        "id": "SMtK2hwCXbnk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f10bbdd4-21aa-4cbf-a598-ebede6f0b442"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['input']=in_df\n",
        "df['output']=out_df"
      ],
      "metadata": {
        "id": "IxlaobY0JNQo"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "3mC-ZMAWYIrs",
        "outputId": "a648b570-3b7c-4919-d9cd-0d4dcc4ec93f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                       input                      output\n",
              "0                 [start, 9, may, 1630, end]  [start, 1630, 05, 09, end]\n",
              "1                 [start, 15, 03, 2014, end]  [start, 2014, 03, 15, end]\n",
              "2                [start, mar, 16, 1675, end]  [start, 1675, 03, 16, end]\n",
              "3                [start, jun, 16, 1640, end]  [start, 1640, 06, 16, end]\n",
              "4          [start, friday, 1791, 2, 09, end]  [start, 1791, 09, 02, end]\n",
              "...                                      ...                         ...\n",
              "39994       [start, december, 26, 1900, end]  [start, 1900, 12, 26, end]\n",
              "39995            [start, 15, may, 1828, end]  [start, 1828, 05, 15, end]\n",
              "39996  [start, friday, april, 18, 1851, end]  [start, 1851, 04, 18, end]\n",
              "39997           [start, june, 11, 2070, end]  [start, 2070, 06, 11, end]\n",
              "39998        [start, january, 27, 1712, end]  [start, 1712, 01, 27, end]\n",
              "\n",
              "[39999 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c1138da0-c3b6-41ca-803e-58459028b49a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[start, 9, may, 1630, end]</td>\n",
              "      <td>[start, 1630, 05, 09, end]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[start, 15, 03, 2014, end]</td>\n",
              "      <td>[start, 2014, 03, 15, end]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[start, mar, 16, 1675, end]</td>\n",
              "      <td>[start, 1675, 03, 16, end]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[start, jun, 16, 1640, end]</td>\n",
              "      <td>[start, 1640, 06, 16, end]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[start, friday, 1791, 2, 09, end]</td>\n",
              "      <td>[start, 1791, 09, 02, end]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39994</th>\n",
              "      <td>[start, december, 26, 1900, end]</td>\n",
              "      <td>[start, 1900, 12, 26, end]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39995</th>\n",
              "      <td>[start, 15, may, 1828, end]</td>\n",
              "      <td>[start, 1828, 05, 15, end]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39996</th>\n",
              "      <td>[start, friday, april, 18, 1851, end]</td>\n",
              "      <td>[start, 1851, 04, 18, end]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39997</th>\n",
              "      <td>[start, june, 11, 2070, end]</td>\n",
              "      <td>[start, 2070, 06, 11, end]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39998</th>\n",
              "      <td>[start, january, 27, 1712, end]</td>\n",
              "      <td>[start, 1712, 01, 27, end]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>39999 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1138da0-c3b6-41ca-803e-58459028b49a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c1138da0-c3b6-41ca-803e-58459028b49a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c1138da0-c3b6-41ca-803e-58459028b49a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e9cd437f-62e9-41a7-8829-68b3598111eb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e9cd437f-62e9-41a7-8829-68b3598111eb')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e9cd437f-62e9-41a7-8829-68b3598111eb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "JyFPYonWTYvV"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "UWmoAjt8QK4V"
      },
      "outputs": [],
      "source": [
        "def create_tokenizer(lines):\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = Tokenizer()\n",
        "\n",
        "    # Fit the tokenizer on the text provided\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "\n",
        "    # Return the tokenizer\n",
        "    return tokenizer\n",
        "# Creating tokenizer instances for English text\n",
        "tokenizer_input = create_tokenizer(df['input'])\n",
        "\n",
        "# Creating tokenizer instances for French text\n",
        "tokenizer_output = create_tokenizer(df['output'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "RDZCZkUXTb2f"
      },
      "outputs": [],
      "source": [
        "vocab_size_input = len(tokenizer_input.word_index) + 1\n",
        "\n",
        "# For French\n",
        "vocab_size_output = len(tokenizer_output.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOsZoPmhToIf",
        "outputId": "6d1f15b7-0a72-4e71-c099-33f2607d561d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "700"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "vocab_size_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "_rPUMPEwTuGo"
      },
      "outputs": [],
      "source": [
        "# Function to compute maximum length of tokenized forms\n",
        "def max_length(lines):\n",
        "\n",
        "    # Compute the maximum length amongst all sentences in text\n",
        "    return max([len(s.split()) for s in lines])\n",
        "\n",
        "# Storing the maximum length for English\n",
        "length_input = 10\n",
        "\n",
        "# Storing the maximum length for French\n",
        "length_output = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "KfBOc_79T20W"
      },
      "outputs": [],
      "source": [
        "# Function that will tokenize and pad the text\n",
        "def encode_text(tokenizer, lines, length):\n",
        "\n",
        "    # integer encode\n",
        "    encoded = tokenizer.texts_to_sequences(lines)\n",
        "\n",
        "    # pad encoded sequences\n",
        "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "\n",
        "    return padded\n",
        "\n",
        "# Apply the tokenization and padding on English Text\n",
        "input = encode_text(tokenizer_input, df['input'], length_input)\n",
        "\n",
        "# Apply the tokenization and padding on French Text\n",
        "output = encode_text(tokenizer_output, df['output'], length_output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(input,input.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6HOIoHwn1el",
        "outputId": "9439d02c-6b30-40ea-af81-f628e2f912af"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1 28  3 ...  0  0  0]\n",
            " [ 1 37 71 ...  0  0  0]\n",
            " [ 1 53 27 ...  0  0  0]\n",
            " ...\n",
            " [ 1 51  9 ...  0  0  0]\n",
            " [ 1 13 17 ...  0  0  0]\n",
            " [ 1  4 32 ...  0  0  0]] (39999, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "gOg3EIxxminT"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test=train_test_split(input,output,train_size=0.8,random_state=42)\n"
      ],
      "metadata": {
        "id": "OT6UQF7emre_"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input=X_train\n",
        "output=y_train"
      ],
      "metadata": {
        "id": "vCUFwJkfnBaK"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJwimrJoUFCJ",
        "outputId": "2d75b370-4a9c-4c96-9083-d7978a5f4df0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1 20  7 ...  0  0  0]\n",
            " [ 1 31  4 ...  0  0  0]\n",
            " [ 1 46  7 ...  0  0  0]\n",
            " ...\n",
            " [ 1 48 14 ...  0  0  0]\n",
            " [ 1 28 44 ...  0  0  0]\n",
            " [ 1  8 16 ...  0  0  0]] 159995\n"
          ]
        }
      ],
      "source": [
        "print(input,output.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Sbjya0bKcUa0"
      },
      "outputs": [],
      "source": [
        "class Encoder(Model):\n",
        "\n",
        "    # Intialize Model's Instance's parameters\n",
        "    # the encoder parameters intialized above are taken as the default parameters for encoder\n",
        "\n",
        "    def __init__(self, inp_vocab_size, inp_embed_size, inp_lstm_cells, batch_size, inp_len):\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.inp_embed_size = inp_embed_size # Initializing size that the Embedding layer outputs\n",
        "\n",
        "        self.inp_vocab_size = inp_vocab_size # Initializing the vocabulary size of source text\n",
        "\n",
        "        self.inp_lstm_cells = inp_lstm_cells # Initializing the number of hidden cells in the lstm layer(no of lstm)\n",
        "\n",
        "        self.batch_size = batch_size         # Initializing the batch size\n",
        "        self.inp_len = inp_len               # Initializing the maximum input length of any tokenized source text\n",
        "\n",
        "\n",
        "        # Initializing an Embedding layer here\n",
        "\n",
        "        self.enc_embedding = Embedding(self.inp_vocab_size, self.inp_embed_size, trainable=True)\n",
        "\n",
        "        # Initializing an LSTM layer here\n",
        "\n",
        "        self.lstm = LSTM(self.inp_lstm_cells, return_sequences=True, return_state=True)\n",
        "\n",
        "\n",
        "    # Use 'call' function to make the Encoder Model produce actions that you would want it to\n",
        "\n",
        "    # Takes the source input sequence as first input and a hidden state as the second input\n",
        "    def call(self, inp_sequence, hidden_sequence):\n",
        "\n",
        "        # First, call the embedding layer defined above to embed the source input sequence\n",
        "        emb_output = self.enc_embedding(inp_sequence)\n",
        "        # emb_output shape = [batch_size, max_source_inp_length, embedding_dim]\n",
        "\n",
        "        inp_lstm_output, state_h, state_c = self.lstm(emb_output, initial_state = hidden_sequence)\n",
        "        # lstm_output shape = [batch_size, max_source_inp_length, lstm_cells_dim]\n",
        "        # state_h shape = [batch_size, lstm_cells_dim]\n",
        "\n",
        "        # 1. lstm_output contains output of the LSTM from all time steps\n",
        "        # 2. state_h contains output from the last time step\n",
        "        # 3. state_c contains the information of the state of the LSTM at the last time step\n",
        "\n",
        "        # return all the outputs at each time step (lstm_output) and the output at the last time step (state_h)\n",
        "        return inp_lstm_output, state_h\n",
        "\n",
        "\n",
        "    # This function is meant to initialize the hidden weights of the lstm layer\n",
        "\n",
        "    def initialize_hidden_states(self):\n",
        "\n",
        "        # There are two sets of initializations made - for the hidden cell input (h at t-1) and hidden cell state (c at t-1)\n",
        "        return [tf.zeros([self.batch_size, self.inp_lstm_cells]), tf.zeros([self.batch_size, self.inp_lstm_cells])]\n",
        "# Initialize an instance of the Encoder mo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "hpCU_1jKdo-S"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "# The source language's tokenized text will be embedded with this length\n",
        "inp_embed_size = 128\n",
        "# Number of hidden cells in the LSTM layer that learns sequences of source language\n",
        "inp_lstm_cells = 256\n",
        "\n",
        "# The target language's tokenized text will be embedded with this length\n",
        "tar_embed_size = 128\n",
        "# Number of hidden cells in the LSTM layer that learns sequences of target language\n",
        "tar_lstm_cells = 256\n",
        "\n",
        "# Number of attention units (will discuss about it now)\n",
        "attention_units = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "UcB9ymE2dhi_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a970b725-7e37-48c3-da4b-db0e5b904eae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inp_embed_size 128\n",
            "inp_vocab_size 700\n",
            "inp_lstm_cells 256\n"
          ]
        }
      ],
      "source": [
        "# Initialize an instance of the Encoder model above with the defined parameters\n",
        "\n",
        "enc_model = Encoder(inp_vocab_size=vocab_size_input, inp_embed_size=inp_embed_size, inp_lstm_cells=inp_lstm_cells,\n",
        "                    batch_size=batch_size, inp_len=length_input)\n",
        "\n",
        "\n",
        "# Test the working below\n",
        "\n",
        "# Initialize hidden states\n",
        "initialized_hidden_states = enc_model.initialize_hidden_states()\n",
        "\n",
        "# Feed an input and see if the model produces an output\n",
        "enc_out, enc_state = enc_model(input[:batch_size], initialized_hidden_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "kofJ6vNUeE5K"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Attention(Layer):\n",
        "\n",
        "                                        # Default parameters supplied (same as encoder)\n",
        "    def __init__(self, attention_units, bs = batch_size, inp_len = length_input):\n",
        "\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.W1 = Dense(attention_units) # Initialize a Dense layer with neurons = number of attention_units\n",
        "        self.W2 = Dense(attention_units) # Initialize a Dense layer with neurons = number of attention_units\n",
        "        self.V = Dense(1)                 # Initialize a Dense layer with neurons = 1\n",
        "\n",
        "        self.batch_size = bs\n",
        "        self.inp_len = length_input\n",
        "\n",
        "\n",
        "    # This function is executed when an instance of this class is called\n",
        "\n",
        "    # hidden = output of the current target state\n",
        "    # output = all outputs of the encoder model\n",
        "    def call(self, hidden, output):\n",
        "\n",
        "        # the shape of the hidden input is extended i.e. from [batch_size, tar_lstm_cells] to [batch_size, 1, tar_lstm_cells]\n",
        "        hidden_extended = tf.expand_dims(hidden, axis=1)\n",
        "\n",
        "        # hidden_extended shape = [batch_size, 1, target_lstm_cells_dim]\n",
        "        # output shape = [batch_size, max_source_inp_length, source_lstm_cells_dim]\n",
        "\n",
        "        # Formula 4 i.e. Bahdanau Attention applied here to generate a score\n",
        "        score = self.V(tf.nn.tanh(self.W1(hidden_extended) + self.W2(output)))\n",
        "        # score shape = [batch_size, max_source_inp_length, 1]                                     #### check this\n",
        "\n",
        "        # Formula 1 applied here to get the attention weights (notice axis = 1)\n",
        "        # since we have to compute a weight to each source input, the softmax is applied at axis=1 of score\n",
        "        atn_weights = tf.nn.softmax(score, axis=1)\n",
        "        # attention_weights shape = [batch_size, max_source_inp_length(128), 1]                         #### check this\n",
        "\n",
        "        # A part of Formula 2 is applied where we multiple the attention weights with the outputs from the encoder model\n",
        "        context_vector = atn_weights*output\n",
        "        # context_vector shape = [batch_size, max_source_inp_length, source_lstm_cells]\n",
        "\n",
        "        # The second part of Formula 2 where the multiplied product is summed against each source input\n",
        "        # the axis=1 here has the same reasoning wherein we want to sum the values to get one vector for each source input\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        # context_vector shape = [batch_size, source_lstm_cells]\n",
        "\n",
        "        # return the context_vector and the attention_weights generated from the above operations\n",
        "        return context_vector, atn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "CbSnJyIQesSk"
      },
      "outputs": [],
      "source": [
        "class Decoder(Model):\n",
        "\n",
        "                        # Providing parameters to decoder to generate layers\n",
        "    def __init__(self, tar_embed_size, tar_vocab_size, tar_lstm_cells, attention_units,\n",
        "                 batch_size=batch_size, tar_len=length_output):\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.tar_embed_size = tar_embed_size # Initializing size that the Embedding layer outputs\n",
        "        self.tar_vocab_size = tar_vocab_size # Initializing the vocabulary size of target text\n",
        "        self.tar_lstm_cells = tar_lstm_cells # Initializing the number of hidden cells in the lstm layer\n",
        "        self.batch_size = batch_size         # Initializing the batch size\n",
        "        self.tar_len = tar_len               # Initializing the maximum input length of any tokenized target text\n",
        "        self.attention_units = attention_units  # Initializing the number of units for the attention layer\n",
        "\n",
        "\n",
        "        # Initializing an Embedding layer here\n",
        "\n",
        "        # vocab_size given as the range within which it has to work\n",
        "        # embed_size denoting the embedding output size\n",
        "        self.dec_embedding = Embedding(self.tar_vocab_size, self.tar_embed_size, trainable=True)\n",
        "\n",
        "\n",
        "        # Initializing an LSTM layer here\n",
        "\n",
        "        # lstm_cells denotes the number of hidden cells inside the layer\n",
        "        # return_sequences and return_state give out output of LSTM cell and its state at each time step\n",
        "        self.lstm = LSTM(self.tar_lstm_cells, return_sequences=True, return_state=True)\n",
        "\n",
        "\n",
        "        # Initializing an Attention layer here\n",
        "\n",
        "        # attention_units denotes the number of neurons inside the layer\n",
        "        self.attention = Attention(self.attention_units)\n",
        "\n",
        "\n",
        "        # Initializing a Final Dense layer here\n",
        "\n",
        "        # takes the same number or neurons as the vocabulary size of target text (producing a probability value for each word)\n",
        "        self.final_layer = Dense(self.tar_vocab_size)\n",
        "\n",
        "\n",
        "    # This call function is executed when an instance of this class is called\n",
        "\n",
        "    # x = input to the decoder\n",
        "    # hidden = output of the current target state\n",
        "    # enc_output = all outputs of the encoder model\n",
        "    def call(self, x, hidden, enc_output):\n",
        "\n",
        "        # Applying the attention on hidden state of current target state and output from the encoder\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        # context_vector shape = [batch_size, lstm_cells]\n",
        "        # attention_weights shape = [batch_size, max_source_inp_length, 1]                         #### check this\n",
        "\n",
        "        # Apply embeddings on the decoder input\n",
        "        emb_output = self.dec_embedding(x)\n",
        "        # emb_output shape = [batch_size, 1, embedding_size]\n",
        "\n",
        "        # Performing part 1 of Formula 3 i.e. concatenation of context_vector and embedding output\n",
        "        # context_vector shape is expanded first from [batch_size, source_lstm_cells] to [batch_size, 1, source_lstm_cells]\n",
        "        x_context = tf.concat([tf.expand_dims(context_vector, axis=1), emb_output], axis=-1)\n",
        "        # x_context shape = [batch_size, 1, lstm_cells+embedding_size]\n",
        "\n",
        "        # Performing part 2 of Formula 3 i.e. activating it using as LSTM layer\n",
        "        # Running the x_context through as LSTM layer that learns the sequences\n",
        "        tar_lstm_output, tar_state_h, tar_state_c = self.lstm(x_context)\n",
        "        # lstm_output shape = [batch_size, 1, lstm_cells]\n",
        "        # state_h shape = [batch_size, lstm_cells]\n",
        "        # state_c shape = [batch_size, lstm_cells]\n",
        "\n",
        "        # The output is reshaped here in order to be fed to the final Dense layer\n",
        "        tar_lstm_output_reshaped = tf.reshape(tar_lstm_output, shape=(-1, tar_lstm_output.shape[2]))\n",
        "        # tar_lstm_output_reshaped shape = [batch_size, lstm_cells]\n",
        "\n",
        "        # the final dense layer outputs a score for each word that may be considered as a prediction\n",
        "        # for this reason, the complete target vocabulary is provided as number of neurons to this layer\n",
        "        word_prob = self.final_layer(tar_lstm_output_reshaped)\n",
        "        # word_prob shape = [batch_size, tar_vocab_size]\n",
        "\n",
        "        # return the word predictions, last output from the decoder LSTM and the attention_weights\n",
        "        return word_prob, tar_state_h, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "pwKUE0Aiext9"
      },
      "outputs": [],
      "source": [
        "dec_model = Decoder(tar_embed_size=tar_embed_size, tar_vocab_size=vocab_size_output, tar_lstm_cells=tar_lstm_cells,\n",
        "                    attention_units=attention_units)\n",
        "\n",
        "# Test the working below\n",
        "\n",
        "dec_out, dec_state, atn_w = dec_model(tf.random.uniform((batch_size, 1)), enc_state, enc_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "jqd5hKPMe8GM"
      },
      "outputs": [],
      "source": [
        "# Adam optimizer will be implemented\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# An instance of the SparseCategoricalCrossentropy Loss with some required arguments that define how it has to perform\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "CM63gLW-fAeL"
      },
      "outputs": [],
      "source": [
        "# Initializing Loss function here\n",
        "def loss_function(real, pred):\n",
        "\n",
        "    # mask shape = [batch_size, ]\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0)) # tf.math.equal returns True when values are equal to 0\n",
        "                                                #tf.math.logical_not inverses the above (True<==>False)\n",
        "\n",
        "    # The mask is meant to only take losses where there is an actual word that exists i.e.\n",
        "    # not for cases where the padding has been placed to put 0 as a token\n",
        "\n",
        "    # Compute loss using real and pred values\n",
        "    loss_ = loss_object(real, pred)\n",
        "    # loss_ shape = [batch_size, ]\n",
        "\n",
        "    # casts mask variable as a tf loss type variable\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "\n",
        "    # multiplies loss with mask (this operation will nullify any loss values where the padding has been put in place)\n",
        "    loss_ *= mask\n",
        "    # loss_ shape = [batch_size, ]\n",
        "\n",
        "            # tf.reduce_mean computes the mean loss\n",
        "    return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "hIDV5q6_fCpJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Function for shuffling the dataset for fitting the model with (implemented every epoch)\n",
        "def shuffler(lang_inp, lang_out):\n",
        "\n",
        "    n_elem = lang_inp.shape[0]\n",
        "    indices = np.random.choice(n_elem, size=n_elem, replace=False)\n",
        "\n",
        "    return lang_inp[indices], lang_out[indices]\n",
        "\n",
        "\n",
        "# Function for generating a batch of data from the dataset provided\n",
        "def generator(batch_number, lang_input, lang_output):\n",
        "\n",
        "    if len(lang_input) <= batch_number*batch_size+batch_size:\n",
        "\n",
        "        return (lang_input[batch_number*batch_size:],\n",
        "            lang_output[batch_number*batch_size:])\n",
        "\n",
        "    return (lang_input[batch_number*batch_size: batch_number*batch_size+batch_size],\n",
        "            lang_output[batch_number*batch_size: batch_number*batch_size+batch_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "hkdOPJKNfNMC"
      },
      "outputs": [],
      "source": [
        "# Function to call for training every batch\n",
        "\n",
        "@tf.function  # Decorator - Compiles the function below into a callable TensorFlow graph\n",
        "def train_step(inp, targ, enc_hidden): # runs for a single batch\n",
        "\n",
        "    # initialize loss as 0 for the batch\n",
        "    loss = 0\n",
        "\n",
        "    # uses gradient tape so that differentiation takes place accordingly to update gradient and weights\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        # get the output from encoder model\n",
        "        enc_output, enc_hidden = enc_model(inp, enc_hidden)\n",
        "        # enc_output shape = [batch_size, max_source_inp_length, lstm_cells]\n",
        "        # enc_hidden shape = [batch_size, lstm_cells]\n",
        "\n",
        "        # assign enc_hidden state as dec_hidden state\n",
        "        dec_hidden = enc_hidden\n",
        "        # dec_hidden shape = [batch_size, lstm_cells]\n",
        "\n",
        "        # for each sample in batch take starting index as the same as 'start' taken from target tokenizer\n",
        "        # use the above as the first input to the decoder model\n",
        "        dec_input = tf.expand_dims([tokenizer_output.word_index['start']] * batch_size, 1)\n",
        "        # dec_input shape = [batch_size, 1]\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]): # run for the complete length of target outputs\n",
        "\n",
        "          # passing enc_output to the decoder\n",
        "          predictions, dec_hidden, _ = dec_model(dec_input, dec_hidden, enc_output)\n",
        "          # predictions shape = [batch_size, tar_vocab_size]\n",
        "          # dec_hidden shape = [batch_size, lstm_cells]\n",
        "          # targ[:, t] shape = [batch_size, ]\n",
        "\n",
        "          # compute loss for the word predicted (t) after the previous word supplied for all samples\n",
        "          loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "          # using teacher forcing (take the next word as an input to the decoder model)\n",
        "          dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "          # dec_input shape = [batch_size, 1]\n",
        "\n",
        "    # computing batch loss by dividing the total loss with the length of target\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "\n",
        "    # As the loss has now been computed, let's notify the model to optimize the gradients\n",
        "\n",
        "    # collecting all variables to compute gradient\n",
        "    variables = enc_model.trainable_variables + dec_model.trainable_variables\n",
        "\n",
        "    # compute the gradients. (loss is differentiated against variables)\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    # Applies gradients to variables\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDAOZGMofQmC",
        "outputId": "87b4291c-5af1-42e2-8153-fd5acfd24187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 5.0972\n",
            "Epoch 1 Batch 200 Loss 2.4632\n",
            "Epoch 1 Batch 400 Loss 2.1736\n",
            "Epoch 1 Batch 600 Loss 1.8890\n",
            "Epoch 1 Batch 800 Loss 1.7666\n",
            "Epoch 1 Loss 2.1774\n",
            "Time taken for 1 epoch 29.944136381149292 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.6039\n",
            "Epoch 2 Batch 200 Loss 1.5213\n",
            "Epoch 2 Batch 400 Loss 1.4076\n",
            "Epoch 2 Batch 600 Loss 1.3844\n",
            "Epoch 2 Batch 800 Loss 1.3406\n",
            "Epoch 2 Loss 1.4474\n",
            "Time taken for 1 epoch 11.238277912139893 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.3092\n",
            "Epoch 3 Batch 200 Loss 1.3917\n",
            "Epoch 3 Batch 400 Loss 1.3230\n",
            "Epoch 3 Batch 600 Loss 1.2934\n",
            "Epoch 3 Batch 800 Loss 1.3181\n",
            "Epoch 3 Loss 1.3306\n",
            "Time taken for 1 epoch 11.668607473373413 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.2680\n",
            "Epoch 4 Batch 200 Loss 1.2783\n",
            "Epoch 4 Batch 400 Loss 1.2649\n",
            "Epoch 4 Batch 600 Loss 1.2661\n",
            "Epoch 4 Batch 800 Loss 1.2602\n",
            "Epoch 4 Loss 1.2884\n",
            "Time taken for 1 epoch 11.437989473342896 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.2651\n",
            "Epoch 5 Batch 200 Loss 1.2512\n",
            "Epoch 5 Batch 400 Loss 1.2826\n",
            "Epoch 5 Batch 600 Loss 1.2284\n",
            "Epoch 5 Batch 800 Loss 1.1954\n",
            "Epoch 5 Loss 1.2143\n",
            "Time taken for 1 epoch 10.959178924560547 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.1568\n",
            "Epoch 6 Batch 200 Loss 1.0986\n",
            "Epoch 6 Batch 400 Loss 1.1030\n",
            "Epoch 6 Batch 600 Loss 1.0157\n",
            "Epoch 6 Batch 800 Loss 1.0250\n",
            "Epoch 6 Loss 1.0415\n",
            "Time taken for 1 epoch 10.728248596191406 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.9149\n",
            "Epoch 7 Batch 200 Loss 0.8527\n",
            "Epoch 7 Batch 400 Loss 0.8050\n",
            "Epoch 7 Batch 600 Loss 0.7754\n",
            "Epoch 7 Batch 800 Loss 0.5697\n",
            "Epoch 7 Loss 0.7496\n",
            "Time taken for 1 epoch 12.166343212127686 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.6363\n",
            "Epoch 8 Batch 200 Loss 0.3970\n",
            "Epoch 8 Batch 400 Loss 0.2669\n",
            "Epoch 8 Batch 600 Loss 0.3409\n",
            "Epoch 8 Batch 800 Loss 0.1831\n",
            "Epoch 8 Loss 0.3048\n",
            "Time taken for 1 epoch 10.689066648483276 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0783\n",
            "Epoch 9 Batch 200 Loss 0.1115\n",
            "Epoch 9 Batch 400 Loss 0.0839\n",
            "Epoch 9 Batch 600 Loss 0.0375\n",
            "Epoch 9 Batch 800 Loss 0.1048\n",
            "Epoch 9 Loss 0.1017\n",
            "Time taken for 1 epoch 12.315739870071411 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0593\n",
            "Epoch 10 Batch 200 Loss 0.0190\n",
            "Epoch 10 Batch 400 Loss 0.0140\n",
            "Epoch 10 Batch 600 Loss 0.0168\n",
            "Epoch 10 Batch 800 Loss 0.0160\n",
            "Epoch 10 Loss 0.0634\n",
            "Time taken for 1 epoch 14.204894065856934 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.0377\n",
            "Epoch 11 Batch 200 Loss 0.1363\n",
            "Epoch 11 Batch 400 Loss 0.0112\n",
            "Epoch 11 Batch 600 Loss 0.0161\n",
            "Epoch 11 Batch 800 Loss 0.1311\n",
            "Epoch 11 Loss 0.0499\n",
            "Time taken for 1 epoch 12.624412059783936 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.0113\n",
            "Epoch 12 Batch 200 Loss 0.1027\n",
            "Epoch 12 Batch 400 Loss 0.0874\n",
            "Epoch 12 Batch 600 Loss 0.0052\n",
            "Epoch 12 Batch 800 Loss 0.1052\n",
            "Epoch 12 Loss 0.0412\n",
            "Time taken for 1 epoch 11.132531881332397 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.0184\n",
            "Epoch 13 Batch 200 Loss 0.0416\n",
            "Epoch 13 Batch 400 Loss 0.0263\n",
            "Epoch 13 Batch 600 Loss 0.0282\n",
            "Epoch 13 Batch 800 Loss 0.1352\n",
            "Epoch 13 Loss 0.0325\n",
            "Time taken for 1 epoch 10.992051362991333 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.0047\n",
            "Epoch 14 Batch 200 Loss 0.0034\n",
            "Epoch 14 Batch 400 Loss 0.0421\n",
            "Epoch 14 Batch 600 Loss 0.0534\n",
            "Epoch 14 Batch 800 Loss 0.0343\n",
            "Epoch 14 Loss 0.0277\n",
            "Time taken for 1 epoch 10.45121169090271 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.0356\n",
            "Epoch 15 Batch 200 Loss 0.0021\n",
            "Epoch 15 Batch 400 Loss 0.0021\n",
            "Epoch 15 Batch 600 Loss 0.0245\n",
            "Epoch 15 Batch 800 Loss 0.0146\n",
            "Epoch 15 Loss 0.0221\n",
            "Time taken for 1 epoch 10.380502700805664 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.0297\n",
            "Epoch 16 Batch 200 Loss 0.0078\n",
            "Epoch 16 Batch 400 Loss 0.0313\n",
            "Epoch 16 Batch 600 Loss 0.0020\n",
            "Epoch 16 Batch 800 Loss 0.0513\n",
            "Epoch 16 Loss 0.0179\n",
            "Time taken for 1 epoch 11.145079851150513 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0034\n",
            "Epoch 17 Batch 200 Loss 0.0040\n",
            "Epoch 17 Batch 400 Loss 0.0024\n",
            "Epoch 17 Batch 600 Loss 0.0271\n",
            "Epoch 17 Batch 800 Loss 0.0053\n",
            "Epoch 17 Loss 0.0119\n",
            "Time taken for 1 epoch 12.905887842178345 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0205\n",
            "Epoch 18 Batch 200 Loss 0.0026\n",
            "Epoch 18 Batch 400 Loss 0.0026\n",
            "Epoch 18 Batch 600 Loss 0.0050\n",
            "Epoch 18 Batch 800 Loss 0.0091\n",
            "Epoch 18 Loss 0.0103\n",
            "Time taken for 1 epoch 10.809921979904175 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0043\n",
            "Epoch 19 Batch 200 Loss 0.0159\n",
            "Epoch 19 Batch 400 Loss 0.0340\n",
            "Epoch 19 Batch 600 Loss 0.0143\n",
            "Epoch 19 Batch 800 Loss 0.0010\n",
            "Epoch 19 Loss 0.0097\n",
            "Time taken for 1 epoch 10.430538654327393 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0031\n",
            "Epoch 20 Batch 200 Loss 0.0303\n",
            "Epoch 20 Batch 400 Loss 0.0185\n",
            "Epoch 20 Batch 600 Loss 0.0114\n",
            "Epoch 20 Batch 800 Loss 0.0134\n",
            "Epoch 20 Loss 0.0071\n",
            "Time taken for 1 epoch 10.080535650253296 sec\n",
            "\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 20 # Initialize the number of epochs to run\n",
        "\n",
        "# To store loss of each epoch\n",
        "loss_history = []\n",
        "\n",
        "# Computing number of batches\n",
        "number_of_batches = input.shape[0]//batch_size\n",
        "\n",
        "# Splitting the dataset at the end to form full batches\n",
        "lang_input_split = input[:number_of_batches*batch_size]\n",
        "lang_output_split = output[:number_of_batches*batch_size]\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # initialize hidden states of encoder for the epoch\n",
        "    enc_hidden = enc_model.initialize_hidden_states()\n",
        "\n",
        "    # total loss is 0 for the epoch\n",
        "    total_loss = 0\n",
        "\n",
        "    # Shuffle the dataset here\n",
        "    lang_inp, lang_out = shuffler(lang_input_split, lang_output_split)\n",
        "\n",
        "    # run for all batches of inputs and targets\n",
        "    for batch_number in range(number_of_batches):\n",
        "\n",
        "        # Generate the batch to be sent to the model for training\n",
        "        inp, targ = generator(batch_number, lang_inp, lang_out)\n",
        "\n",
        "        # apply the train step function declared earlier and get the loss for the batch\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "\n",
        "        # add the batch loss to total loss for the epoch\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        # printing some summaries inside the loop\n",
        "        if batch_number % 200 == 0:\n",
        "              print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                       batch_number,\n",
        "                                                       batch_loss.numpy()))\n",
        "\n",
        "    # Appending the total loss of the epoch here\n",
        "    loss_history.append(total_loss / number_of_batches)\n",
        "\n",
        "\n",
        "    # printing epoch summaries here\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / number_of_batches))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "\n",
        "    # The below set of checks are made to ensure that loss is reducing with each epoch. It it is not, break out of the loop\n",
        "    if epoch >= 2:\n",
        "        if loss_history[-1] >= loss_history[-2]: #patience is 0\n",
        "            print(\"breaking now as loss isn't reducing\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to translate the sentence\n",
        "\n",
        "def evaluate(sentence):\n",
        "\n",
        "\n",
        "    # Convert the datatype to tensorflow-tensor\n",
        "    inputs = tf.convert_to_tensor(sentence)\n",
        "\n",
        "    # initialize to store the result\n",
        "    result = ''\n",
        "\n",
        "    # initialize hidden weights of encoder\n",
        "    hidden = [tf.zeros([1, inp_lstm_cells]), tf.zeros([1, inp_lstm_cells])]\n",
        "\n",
        "    # get the output of the encoder using initialized weights\n",
        "    enc_out, enc_hidden = enc_model(inputs, hidden)\n",
        "\n",
        "    # initialize decoder hidden weights same as encoder hidden weights\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    # intialize the decoder input as the tokenized form of 'start'\n",
        "    dec_input = tf.expand_dims([tokenizer_output.word_index['start']], 0)\n",
        "\n",
        "    # run for the maximum length of target language\n",
        "    for t in range(length_output):\n",
        "\n",
        "        # get the output of the encoder based on the outputs provided from encoder\n",
        "        predictions, dec_hidden, attention_weights = dec_model(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # get the argument(token) for the word that is given maximum probability score by the model\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        # check if 'end' is hit on the outputted word from decoder model\n",
        "        if tokenizer_output.index_word[predicted_id] == 'end':\n",
        "            return result, sentence\n",
        "\n",
        "        # Get the word based on the token and add to result\n",
        "\n",
        "        result += tokenizer_output.index_word[predicted_id]+ \" \"\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence"
      ],
      "metadata": {
        "id": "qeD2lnXIQgcN"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_act=[]\n",
        "for input in y_test:\n",
        "  t=\"\"\n",
        "  t=t+tokenizer_output.index_word[input[1]]+\" \"\n",
        "  t=t+tokenizer_output.index_word[input[2]]+\" \"\n",
        "  t=t+tokenizer_output.index_word[input[3]]+\" \"\n",
        "  y_act.append(t)\n"
      ],
      "metadata": {
        "id": "ufmXgj3h9IEy"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xfMNoD-eCxWr"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_act"
      ],
      "metadata": {
        "id": "dqX0N1GGCUMA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "096606f9-e3d9-4b45-ed11-a4e3b7e70445"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1710 09 15 ',\n",
              " '1841 06 08 ',\n",
              " '1565 04 14 ',\n",
              " '2071 08 03 ',\n",
              " '1529 02 06 ',\n",
              " '1594 07 30 ',\n",
              " '1581 05 08 ',\n",
              " '1856 01 24 ',\n",
              " '2012 04 29 ',\n",
              " '1538 06 02 ',\n",
              " '1527 05 31 ',\n",
              " '1986 03 25 ',\n",
              " '1542 02 10 ',\n",
              " '2023 09 17 ',\n",
              " '1980 03 23 ',\n",
              " '1877 12 29 ',\n",
              " '1794 03 10 ',\n",
              " '1819 08 15 ',\n",
              " '1766 12 20 ',\n",
              " '1537 07 20 ',\n",
              " '1622 01 11 ',\n",
              " '1946 10 13 ',\n",
              " '1626 11 01 ',\n",
              " '2013 02 02 ',\n",
              " '1807 02 22 ',\n",
              " '2035 11 12 ',\n",
              " '1571 02 22 ',\n",
              " '1881 07 08 ',\n",
              " '1921 04 12 ',\n",
              " '1648 02 08 ',\n",
              " '1780 05 10 ',\n",
              " '1896 04 18 ',\n",
              " '2067 12 20 ',\n",
              " '1990 12 15 ',\n",
              " '1873 05 26 ',\n",
              " '1926 10 28 ',\n",
              " '1596 03 21 ',\n",
              " '1800 07 26 ',\n",
              " '1696 11 22 ',\n",
              " '1524 05 11 ',\n",
              " '1851 02 26 ',\n",
              " '1578 11 14 ',\n",
              " '1763 04 09 ',\n",
              " '1707 09 15 ',\n",
              " '1534 01 04 ',\n",
              " '1695 12 06 ',\n",
              " '1572 08 21 ',\n",
              " '1908 08 12 ',\n",
              " '1741 09 17 ',\n",
              " '1810 02 25 ',\n",
              " '1571 03 01 ',\n",
              " '1956 07 17 ',\n",
              " '1715 06 23 ',\n",
              " '1627 01 02 ',\n",
              " '1626 03 18 ',\n",
              " '1884 11 13 ',\n",
              " '2015 07 01 ',\n",
              " '2068 02 01 ',\n",
              " '2056 12 31 ',\n",
              " '1663 07 17 ',\n",
              " '1767 11 03 ',\n",
              " '1640 08 24 ',\n",
              " '1697 01 23 ',\n",
              " '1653 04 19 ',\n",
              " '1599 08 25 ',\n",
              " '1787 06 05 ',\n",
              " '1570 08 04 ',\n",
              " '1533 10 21 ',\n",
              " '2007 10 26 ',\n",
              " '1707 10 17 ',\n",
              " '1826 05 18 ',\n",
              " '1781 02 21 ',\n",
              " '1830 06 23 ',\n",
              " '1665 03 17 ',\n",
              " '1976 06 02 ',\n",
              " '1696 11 07 ',\n",
              " '2063 02 24 ',\n",
              " '1792 09 05 ',\n",
              " '1740 06 04 ',\n",
              " '1809 07 24 ',\n",
              " '1591 04 19 ',\n",
              " '1599 01 07 ',\n",
              " '1825 06 19 ',\n",
              " '1602 01 11 ',\n",
              " '1815 08 17 ',\n",
              " '1628 10 05 ',\n",
              " '1858 09 11 ',\n",
              " '2021 05 05 ',\n",
              " '1623 05 13 ',\n",
              " '1646 01 20 ',\n",
              " '1746 06 17 ',\n",
              " '1756 12 09 ',\n",
              " '1690 05 03 ',\n",
              " '1912 11 03 ',\n",
              " '1939 12 01 ',\n",
              " '1951 11 11 ',\n",
              " '1886 01 19 ',\n",
              " '2031 06 21 ',\n",
              " '1561 02 28 ',\n",
              " '1789 12 18 ',\n",
              " '1788 02 05 ',\n",
              " '1957 11 02 ',\n",
              " '1994 11 16 ',\n",
              " '1769 12 24 ',\n",
              " '1572 05 31 ',\n",
              " '2065 08 02 ',\n",
              " '1796 12 06 ',\n",
              " '1746 11 07 ',\n",
              " '1981 04 01 ',\n",
              " '1947 11 28 ',\n",
              " '1924 01 05 ',\n",
              " '1841 12 05 ',\n",
              " '1791 08 17 ',\n",
              " '1544 02 06 ',\n",
              " '1884 06 15 ',\n",
              " '2026 07 18 ',\n",
              " '1808 05 03 ',\n",
              " '1796 11 06 ',\n",
              " '1865 12 26 ',\n",
              " '1980 06 14 ',\n",
              " '1605 07 23 ',\n",
              " '1920 12 10 ',\n",
              " '1935 04 10 ',\n",
              " '2044 11 09 ',\n",
              " '1672 11 26 ',\n",
              " '1911 12 25 ',\n",
              " '1663 11 02 ',\n",
              " '2064 08 12 ',\n",
              " '1638 09 13 ',\n",
              " '1712 09 15 ',\n",
              " '2043 07 25 ',\n",
              " '1914 09 23 ',\n",
              " '1925 05 10 ',\n",
              " '1781 12 26 ',\n",
              " '1655 10 04 ',\n",
              " '1727 09 13 ',\n",
              " '2028 03 21 ',\n",
              " '1906 02 13 ',\n",
              " '1993 06 06 ',\n",
              " '1960 10 24 ',\n",
              " '1814 10 06 ',\n",
              " '1904 08 04 ',\n",
              " '1890 09 01 ',\n",
              " '2039 04 05 ',\n",
              " '1554 01 04 ',\n",
              " '1638 12 17 ',\n",
              " '1978 06 12 ',\n",
              " '1952 07 31 ',\n",
              " '1606 12 03 ',\n",
              " '1807 06 22 ',\n",
              " '1911 12 12 ',\n",
              " '1773 11 15 ',\n",
              " '2032 12 26 ',\n",
              " '1953 04 22 ',\n",
              " '1790 10 22 ',\n",
              " '1955 02 16 ',\n",
              " '1634 02 12 ',\n",
              " '1784 12 12 ',\n",
              " '1658 07 16 ',\n",
              " '1771 03 23 ',\n",
              " '1967 10 14 ',\n",
              " '1834 04 25 ',\n",
              " '1968 12 03 ',\n",
              " '1841 04 03 ',\n",
              " '1645 09 17 ',\n",
              " '1690 07 20 ',\n",
              " '1854 06 02 ',\n",
              " '1541 07 27 ',\n",
              " '1531 05 21 ',\n",
              " '1965 07 08 ',\n",
              " '1879 02 05 ',\n",
              " '2033 12 18 ',\n",
              " '1757 06 17 ',\n",
              " '2031 10 03 ',\n",
              " '2051 10 30 ',\n",
              " '1638 03 28 ',\n",
              " '1776 04 10 ',\n",
              " '2001 02 05 ',\n",
              " '1965 09 04 ',\n",
              " '1843 03 03 ',\n",
              " '1828 10 01 ',\n",
              " '1988 08 20 ',\n",
              " '1589 05 24 ',\n",
              " '1904 02 06 ',\n",
              " '2067 11 25 ',\n",
              " '1842 10 01 ',\n",
              " '1734 07 23 ',\n",
              " '1726 05 13 ',\n",
              " '1658 04 03 ',\n",
              " '1859 08 01 ',\n",
              " '1965 06 03 ',\n",
              " '2060 11 21 ',\n",
              " '1989 09 21 ',\n",
              " '1811 09 18 ',\n",
              " '2033 01 03 ',\n",
              " '1699 09 30 ',\n",
              " '1543 01 11 ',\n",
              " '1820 12 30 ',\n",
              " '1992 05 11 ',\n",
              " '1617 11 08 ',\n",
              " '1546 04 05 ',\n",
              " '1897 05 07 ',\n",
              " '1718 04 03 ',\n",
              " '1784 09 30 ',\n",
              " '1603 11 03 ',\n",
              " '1977 07 31 ',\n",
              " '1535 11 19 ',\n",
              " '2050 10 10 ',\n",
              " '2017 02 23 ',\n",
              " '1646 08 18 ',\n",
              " '2011 09 05 ',\n",
              " '1825 07 19 ',\n",
              " '1816 11 09 ',\n",
              " '1592 07 13 ',\n",
              " '1614 07 06 ',\n",
              " '1764 06 28 ',\n",
              " '1599 08 11 ',\n",
              " '1788 10 14 ',\n",
              " '1937 11 05 ',\n",
              " '1899 08 12 ',\n",
              " '1933 07 11 ',\n",
              " '1726 01 28 ',\n",
              " '1871 11 28 ',\n",
              " '1826 03 16 ',\n",
              " '1882 06 27 ',\n",
              " '1642 01 13 ',\n",
              " '2067 06 14 ',\n",
              " '1756 02 10 ',\n",
              " '1693 05 21 ',\n",
              " '2041 03 06 ',\n",
              " '1598 01 03 ',\n",
              " '1911 12 11 ',\n",
              " '1942 10 21 ',\n",
              " '2057 09 23 ',\n",
              " '1636 08 13 ',\n",
              " '1684 12 21 ',\n",
              " '1582 10 08 ',\n",
              " '1917 03 30 ',\n",
              " '1806 07 03 ',\n",
              " '1608 11 08 ',\n",
              " '2041 12 01 ',\n",
              " '2049 03 31 ',\n",
              " '1764 04 22 ',\n",
              " '1878 02 02 ',\n",
              " '1616 11 15 ',\n",
              " '1885 01 07 ',\n",
              " '1671 07 14 ',\n",
              " '1766 09 23 ',\n",
              " '1545 05 10 ',\n",
              " '1837 02 07 ',\n",
              " '1644 09 23 ',\n",
              " '1867 05 13 ',\n",
              " '1817 10 16 ',\n",
              " '1793 02 19 ',\n",
              " '1767 05 03 ',\n",
              " '2045 09 21 ',\n",
              " '1761 11 09 ',\n",
              " '1993 09 07 ',\n",
              " '1671 09 12 ',\n",
              " '1913 04 07 ',\n",
              " '1850 09 07 ',\n",
              " '1919 05 05 ',\n",
              " '2002 06 11 ',\n",
              " '1711 06 20 ',\n",
              " '2026 07 15 ',\n",
              " '1977 02 01 ',\n",
              " '1889 08 13 ',\n",
              " '1533 09 30 ',\n",
              " '1751 03 17 ',\n",
              " '2033 10 15 ',\n",
              " '1984 05 14 ',\n",
              " '1600 11 11 ',\n",
              " '1922 05 17 ',\n",
              " '2024 09 16 ',\n",
              " '2021 07 17 ',\n",
              " '1792 02 13 ',\n",
              " '2013 07 30 ',\n",
              " '1954 06 15 ',\n",
              " '1586 07 09 ',\n",
              " '1966 07 13 ',\n",
              " '1852 05 02 ',\n",
              " '1560 04 30 ',\n",
              " '1677 09 01 ',\n",
              " '1672 02 13 ',\n",
              " '2055 03 19 ',\n",
              " '1619 10 27 ',\n",
              " '1877 09 21 ',\n",
              " '1940 07 07 ',\n",
              " '1594 03 12 ',\n",
              " '1546 01 22 ',\n",
              " '1716 10 11 ',\n",
              " '1532 07 26 ',\n",
              " '1946 11 12 ',\n",
              " '1559 02 20 ',\n",
              " '1609 02 25 ',\n",
              " '2045 01 08 ',\n",
              " '1612 05 19 ',\n",
              " '1885 12 30 ',\n",
              " '1710 05 08 ',\n",
              " '1775 11 26 ',\n",
              " '1591 01 15 ',\n",
              " '2032 11 24 ',\n",
              " '2047 09 09 ',\n",
              " '1678 07 19 ',\n",
              " '1525 02 27 ',\n",
              " '1612 03 30 ',\n",
              " '1810 01 03 ',\n",
              " '1876 02 22 ',\n",
              " '1919 06 17 ',\n",
              " '1553 05 06 ',\n",
              " '1995 05 31 ',\n",
              " '1841 05 30 ',\n",
              " '1873 02 15 ',\n",
              " '1708 04 29 ',\n",
              " '1719 01 28 ',\n",
              " '1832 06 05 ',\n",
              " '1623 07 18 ',\n",
              " '1853 05 27 ',\n",
              " '2045 01 06 ',\n",
              " '1581 09 27 ',\n",
              " '1622 09 18 ',\n",
              " '1537 11 07 ',\n",
              " '1909 02 28 ',\n",
              " '2052 06 05 ',\n",
              " '2028 10 31 ',\n",
              " '1568 06 03 ',\n",
              " '1569 10 18 ',\n",
              " '1962 03 24 ',\n",
              " '1660 06 29 ',\n",
              " '1594 05 26 ',\n",
              " '1957 06 27 ',\n",
              " '2069 01 10 ',\n",
              " '1704 11 18 ',\n",
              " '1835 08 03 ',\n",
              " '1911 10 25 ',\n",
              " '1594 02 03 ',\n",
              " '1841 07 03 ',\n",
              " '2037 03 11 ',\n",
              " '1614 09 10 ',\n",
              " '1871 07 31 ',\n",
              " '1787 11 03 ',\n",
              " '1798 02 12 ',\n",
              " '1529 05 18 ',\n",
              " '1973 02 24 ',\n",
              " '1897 10 29 ',\n",
              " '1660 10 18 ',\n",
              " '2050 03 17 ',\n",
              " '1780 10 03 ',\n",
              " '2023 12 28 ',\n",
              " '1628 05 18 ',\n",
              " '1526 04 18 ',\n",
              " '1649 07 08 ',\n",
              " '1701 02 13 ',\n",
              " '1767 04 21 ',\n",
              " '1883 03 18 ',\n",
              " '1612 01 27 ',\n",
              " '1964 12 07 ',\n",
              " '1883 04 04 ',\n",
              " '1982 02 25 ',\n",
              " '2008 10 26 ',\n",
              " '1556 01 14 ',\n",
              " '1702 04 02 ',\n",
              " '1703 11 26 ',\n",
              " '1799 01 30 ',\n",
              " '1782 06 17 ',\n",
              " '2045 10 25 ',\n",
              " '1793 01 03 ',\n",
              " '1557 03 27 ',\n",
              " '1798 06 23 ',\n",
              " '1763 06 11 ',\n",
              " '1827 06 15 ',\n",
              " '1579 05 06 ',\n",
              " '1997 08 08 ',\n",
              " '1756 02 24 ',\n",
              " '1583 06 30 ',\n",
              " '2015 11 25 ',\n",
              " '1805 09 04 ',\n",
              " '1587 12 16 ',\n",
              " '1591 07 22 ',\n",
              " '1982 08 21 ',\n",
              " '1883 03 24 ',\n",
              " '1943 08 16 ',\n",
              " '1946 08 04 ',\n",
              " '1732 03 16 ',\n",
              " '1646 07 14 ',\n",
              " '1940 02 23 ',\n",
              " '1693 02 25 ',\n",
              " '2041 01 26 ',\n",
              " '1946 06 02 ',\n",
              " '1785 03 05 ',\n",
              " '2045 03 27 ',\n",
              " '1874 06 13 ',\n",
              " '2047 04 26 ',\n",
              " '1530 11 11 ',\n",
              " '1895 07 24 ',\n",
              " '1545 03 09 ',\n",
              " '2030 02 01 ',\n",
              " '1886 10 22 ',\n",
              " '1553 10 08 ',\n",
              " '1595 04 25 ',\n",
              " '1731 09 07 ',\n",
              " '1893 09 19 ',\n",
              " '1627 04 21 ',\n",
              " '1678 12 25 ',\n",
              " '1681 12 14 ',\n",
              " '1682 08 18 ',\n",
              " '2024 04 29 ',\n",
              " '2011 10 07 ',\n",
              " '1877 06 06 ',\n",
              " '1863 04 30 ',\n",
              " '1855 03 17 ',\n",
              " '1580 05 29 ',\n",
              " '2033 10 22 ',\n",
              " '1863 03 08 ',\n",
              " '1695 08 14 ',\n",
              " '1679 08 11 ',\n",
              " '1647 12 02 ',\n",
              " '1622 08 14 ',\n",
              " '1590 11 13 ',\n",
              " '1873 05 01 ',\n",
              " '2009 10 05 ',\n",
              " '1876 08 12 ',\n",
              " '2041 06 20 ',\n",
              " '1653 07 04 ',\n",
              " '1710 09 09 ',\n",
              " '1563 03 17 ',\n",
              " '1768 10 25 ',\n",
              " '1862 05 27 ',\n",
              " '1908 11 22 ',\n",
              " '2050 04 19 ',\n",
              " '1857 08 06 ',\n",
              " '1716 01 08 ',\n",
              " '1875 04 11 ',\n",
              " '1809 08 06 ',\n",
              " '2024 09 17 ',\n",
              " '1739 05 22 ',\n",
              " '1759 01 12 ',\n",
              " '1876 05 19 ',\n",
              " '2001 07 07 ',\n",
              " '2066 07 20 ',\n",
              " '1946 07 05 ',\n",
              " '1571 12 31 ',\n",
              " '1861 05 08 ',\n",
              " '1861 02 03 ',\n",
              " '2056 10 18 ',\n",
              " '1698 10 26 ',\n",
              " '1647 07 11 ',\n",
              " '1645 10 23 ',\n",
              " '1878 02 18 ',\n",
              " '1766 08 24 ',\n",
              " '1601 12 23 ',\n",
              " '2009 12 18 ',\n",
              " '1543 05 22 ',\n",
              " '1822 09 02 ',\n",
              " '1903 04 25 ',\n",
              " '1716 02 12 ',\n",
              " '1953 08 24 ',\n",
              " '1858 06 25 ',\n",
              " '2002 08 02 ',\n",
              " '1790 02 21 ',\n",
              " '1752 01 02 ',\n",
              " '1692 10 31 ',\n",
              " '1722 06 09 ',\n",
              " '1974 06 14 ',\n",
              " '1928 06 11 ',\n",
              " '1547 01 20 ',\n",
              " '2062 06 04 ',\n",
              " '1801 07 24 ',\n",
              " '1957 03 28 ',\n",
              " '1549 09 28 ',\n",
              " '1907 12 14 ',\n",
              " '1988 02 21 ',\n",
              " '1900 01 03 ',\n",
              " '2016 03 30 ',\n",
              " '1678 12 12 ',\n",
              " '1913 03 07 ',\n",
              " '1907 03 11 ',\n",
              " '1711 06 24 ',\n",
              " '1733 05 06 ',\n",
              " '1868 05 20 ',\n",
              " '1753 06 11 ',\n",
              " '1948 02 27 ',\n",
              " '1854 07 07 ',\n",
              " '1555 03 13 ',\n",
              " '1915 12 11 ',\n",
              " '1765 09 01 ',\n",
              " '1745 07 11 ',\n",
              " '1815 02 12 ',\n",
              " '1660 07 05 ',\n",
              " '1908 09 04 ',\n",
              " '1754 08 04 ',\n",
              " '2055 06 03 ',\n",
              " '1792 08 27 ',\n",
              " '2069 09 07 ',\n",
              " '1782 01 09 ',\n",
              " '1724 09 24 ',\n",
              " '1685 06 08 ',\n",
              " '1879 03 20 ',\n",
              " '1559 10 20 ',\n",
              " '1799 10 22 ',\n",
              " '1930 03 26 ',\n",
              " '2013 08 02 ',\n",
              " '1558 05 31 ',\n",
              " '1984 02 09 ',\n",
              " '1529 03 08 ',\n",
              " '1608 04 14 ',\n",
              " '1566 01 14 ',\n",
              " '1692 02 26 ',\n",
              " '2060 10 07 ',\n",
              " '1649 03 24 ',\n",
              " '1788 04 28 ',\n",
              " '1759 03 17 ',\n",
              " '1699 06 12 ',\n",
              " '1617 08 20 ',\n",
              " '1653 04 24 ',\n",
              " '1688 07 23 ',\n",
              " '1847 03 24 ',\n",
              " '1922 08 13 ',\n",
              " '1644 09 17 ',\n",
              " '2014 01 17 ',\n",
              " '1632 10 31 ',\n",
              " '1732 07 24 ',\n",
              " '1781 09 02 ',\n",
              " '1784 11 05 ',\n",
              " '1653 10 29 ',\n",
              " '1904 03 25 ',\n",
              " '1940 08 28 ',\n",
              " '1754 03 26 ',\n",
              " '1756 08 06 ',\n",
              " '1754 04 30 ',\n",
              " '2061 05 01 ',\n",
              " '1690 05 21 ',\n",
              " '1685 02 20 ',\n",
              " '1936 11 05 ',\n",
              " '1607 05 24 ',\n",
              " '1602 04 16 ',\n",
              " '1883 12 18 ',\n",
              " '1788 02 12 ',\n",
              " '1677 01 11 ',\n",
              " '1537 03 16 ',\n",
              " '1959 02 14 ',\n",
              " '1637 11 08 ',\n",
              " '1738 07 08 ',\n",
              " '1833 07 31 ',\n",
              " '1875 03 03 ',\n",
              " '1839 11 25 ',\n",
              " '1859 08 17 ',\n",
              " '1533 01 03 ',\n",
              " '1828 05 15 ',\n",
              " '1610 01 20 ',\n",
              " '1888 09 20 ',\n",
              " '1765 02 10 ',\n",
              " '2032 03 25 ',\n",
              " '1771 09 21 ',\n",
              " '1880 11 16 ',\n",
              " '1640 02 20 ',\n",
              " '1982 12 31 ',\n",
              " '1572 09 09 ',\n",
              " '1527 11 24 ',\n",
              " '1591 08 15 ',\n",
              " '1817 02 01 ',\n",
              " '2047 03 14 ',\n",
              " '1652 07 11 ',\n",
              " '2057 05 22 ',\n",
              " '2069 01 23 ',\n",
              " '1956 01 03 ',\n",
              " '1995 12 02 ',\n",
              " '1913 06 02 ',\n",
              " '1607 08 13 ',\n",
              " '1709 11 30 ',\n",
              " '1663 04 13 ',\n",
              " '1661 08 01 ',\n",
              " '1624 01 14 ',\n",
              " '1766 03 31 ',\n",
              " '1950 02 21 ',\n",
              " '1842 06 26 ',\n",
              " '1933 10 22 ',\n",
              " '1927 10 17 ',\n",
              " '1961 09 19 ',\n",
              " '1641 09 04 ',\n",
              " '1718 04 09 ',\n",
              " '2053 06 23 ',\n",
              " '1652 05 02 ',\n",
              " '1719 07 04 ',\n",
              " '1585 05 10 ',\n",
              " '1824 08 11 ',\n",
              " '1561 09 13 ',\n",
              " '1675 08 15 ',\n",
              " '2029 02 24 ',\n",
              " '1895 12 24 ',\n",
              " '1864 10 07 ',\n",
              " '1777 01 10 ',\n",
              " '1985 11 20 ',\n",
              " '1633 11 01 ',\n",
              " '1976 07 09 ',\n",
              " '1883 03 19 ',\n",
              " '1862 04 07 ',\n",
              " '1647 09 25 ',\n",
              " '1934 10 09 ',\n",
              " '1707 06 12 ',\n",
              " '2008 10 05 ',\n",
              " '1915 04 08 ',\n",
              " '1522 07 14 ',\n",
              " '1718 01 16 ',\n",
              " '1952 05 31 ',\n",
              " '1841 07 08 ',\n",
              " '1943 02 09 ',\n",
              " '2026 08 11 ',\n",
              " '1873 04 26 ',\n",
              " '1765 11 21 ',\n",
              " '1810 08 17 ',\n",
              " '1811 09 14 ',\n",
              " '1676 10 11 ',\n",
              " '2069 08 06 ',\n",
              " '2024 06 28 ',\n",
              " '1922 02 14 ',\n",
              " '1677 01 31 ',\n",
              " '1620 03 05 ',\n",
              " '1892 05 21 ',\n",
              " '1856 08 16 ',\n",
              " '1555 12 28 ',\n",
              " '1588 04 16 ',\n",
              " '1936 06 23 ',\n",
              " '1615 05 24 ',\n",
              " '2036 08 24 ',\n",
              " '1575 07 29 ',\n",
              " '1556 11 28 ',\n",
              " '1919 05 16 ',\n",
              " '1850 03 27 ',\n",
              " '1930 05 21 ',\n",
              " '1605 04 25 ',\n",
              " '1701 02 07 ',\n",
              " '1608 10 13 ',\n",
              " '1993 12 16 ',\n",
              " '1816 07 04 ',\n",
              " '1829 06 22 ',\n",
              " '1669 04 26 ',\n",
              " '1849 06 19 ',\n",
              " '1649 03 30 ',\n",
              " '1918 03 16 ',\n",
              " '1558 11 24 ',\n",
              " '2040 05 12 ',\n",
              " '1886 05 21 ',\n",
              " '1931 10 01 ',\n",
              " '1823 02 19 ',\n",
              " '1972 06 24 ',\n",
              " '1579 04 29 ',\n",
              " '1900 01 31 ',\n",
              " '1540 05 26 ',\n",
              " '1863 06 03 ',\n",
              " '1973 06 12 ',\n",
              " '1797 03 13 ',\n",
              " '1746 11 17 ',\n",
              " '1582 09 28 ',\n",
              " '1844 09 04 ',\n",
              " '2048 07 23 ',\n",
              " '1744 05 19 ',\n",
              " '1733 05 30 ',\n",
              " '2023 02 10 ',\n",
              " '1998 10 06 ',\n",
              " '1854 10 07 ',\n",
              " '1663 05 27 ',\n",
              " '2041 10 10 ',\n",
              " '1556 08 14 ',\n",
              " '1640 05 09 ',\n",
              " '1985 08 16 ',\n",
              " '1689 09 12 ',\n",
              " '1925 10 12 ',\n",
              " '1593 10 08 ',\n",
              " '2054 04 10 ',\n",
              " '1719 07 18 ',\n",
              " '1754 01 04 ',\n",
              " '1664 08 23 ',\n",
              " '1578 01 16 ',\n",
              " '1749 08 04 ',\n",
              " '1860 07 02 ',\n",
              " '1789 10 30 ',\n",
              " '1766 01 20 ',\n",
              " '1995 12 26 ',\n",
              " '1769 07 31 ',\n",
              " '1943 05 27 ',\n",
              " '1551 05 31 ',\n",
              " '1605 09 09 ',\n",
              " '2028 01 13 ',\n",
              " '1706 10 27 ',\n",
              " '1684 12 20 ',\n",
              " '1607 01 24 ',\n",
              " '1612 06 28 ',\n",
              " '1931 01 30 ',\n",
              " '1698 03 29 ',\n",
              " '2014 03 08 ',\n",
              " '1583 11 08 ',\n",
              " '2058 12 02 ',\n",
              " '2016 05 10 ',\n",
              " '1928 01 21 ',\n",
              " '2057 05 05 ',\n",
              " '1985 07 03 ',\n",
              " '1661 04 17 ',\n",
              " '1880 04 24 ',\n",
              " '1721 05 29 ',\n",
              " '1676 02 25 ',\n",
              " '2046 04 26 ',\n",
              " '1859 10 02 ',\n",
              " '1684 08 03 ',\n",
              " '1578 05 18 ',\n",
              " '1569 04 16 ',\n",
              " '1844 12 10 ',\n",
              " '2036 07 06 ',\n",
              " '1567 02 01 ',\n",
              " '1530 08 25 ',\n",
              " '1667 01 25 ',\n",
              " '1996 05 17 ',\n",
              " '2015 03 20 ',\n",
              " '1655 04 23 ',\n",
              " '1961 11 18 ',\n",
              " '1796 10 12 ',\n",
              " '1670 09 20 ',\n",
              " '1716 02 02 ',\n",
              " '1875 03 29 ',\n",
              " '1853 04 15 ',\n",
              " '1904 10 09 ',\n",
              " '1755 11 11 ',\n",
              " '1564 04 09 ',\n",
              " '1574 06 22 ',\n",
              " '1903 09 08 ',\n",
              " '2067 10 26 ',\n",
              " '1548 07 03 ',\n",
              " '1834 06 17 ',\n",
              " '1907 12 30 ',\n",
              " '1938 09 02 ',\n",
              " '2044 05 05 ',\n",
              " '1751 03 16 ',\n",
              " '1878 12 07 ',\n",
              " '1918 05 02 ',\n",
              " '1854 05 23 ',\n",
              " '2046 10 15 ',\n",
              " '1913 01 29 ',\n",
              " '1902 09 12 ',\n",
              " '1553 05 22 ',\n",
              " '1979 01 16 ',\n",
              " '1835 01 07 ',\n",
              " '1890 01 27 ',\n",
              " '1837 08 14 ',\n",
              " '1663 10 23 ',\n",
              " '1757 09 09 ',\n",
              " '1785 07 15 ',\n",
              " '2045 09 18 ',\n",
              " '1985 01 15 ',\n",
              " '1659 07 05 ',\n",
              " '1744 10 20 ',\n",
              " '1632 12 27 ',\n",
              " '1700 10 25 ',\n",
              " '1925 04 13 ',\n",
              " '1969 12 20 ',\n",
              " '1611 06 09 ',\n",
              " '1991 06 17 ',\n",
              " '1854 07 15 ',\n",
              " '1558 08 28 ',\n",
              " '1725 08 29 ',\n",
              " '1723 05 13 ',\n",
              " '1860 03 11 ',\n",
              " '1753 09 27 ',\n",
              " '1947 01 18 ',\n",
              " '1572 12 25 ',\n",
              " '1644 08 08 ',\n",
              " '1771 11 03 ',\n",
              " '1681 03 23 ',\n",
              " '2064 11 26 ',\n",
              " '1817 01 04 ',\n",
              " '1700 10 19 ',\n",
              " '2062 08 16 ',\n",
              " '1790 11 18 ',\n",
              " '1731 06 13 ',\n",
              " '1522 04 11 ',\n",
              " '1942 12 19 ',\n",
              " '1870 08 09 ',\n",
              " '1854 02 12 ',\n",
              " '1833 08 10 ',\n",
              " '1870 12 03 ',\n",
              " '1859 01 10 ',\n",
              " '1961 02 24 ',\n",
              " '1760 04 26 ',\n",
              " '2015 11 02 ',\n",
              " '1818 05 21 ',\n",
              " '1992 12 31 ',\n",
              " '1735 05 16 ',\n",
              " '1581 04 21 ',\n",
              " '1683 12 11 ',\n",
              " '2068 12 22 ',\n",
              " '1889 09 21 ',\n",
              " '2043 05 03 ',\n",
              " '1625 07 24 ',\n",
              " '1944 08 29 ',\n",
              " '2005 02 25 ',\n",
              " '1966 10 17 ',\n",
              " '1712 10 10 ',\n",
              " '1811 11 21 ',\n",
              " '1786 01 27 ',\n",
              " '1880 08 16 ',\n",
              " '1982 04 01 ',\n",
              " '1694 11 26 ',\n",
              " '1839 03 07 ',\n",
              " '2035 06 24 ',\n",
              " '1533 04 09 ',\n",
              " '1969 05 15 ',\n",
              " '2018 05 06 ',\n",
              " '1802 08 27 ',\n",
              " '1643 11 14 ',\n",
              " '2030 05 16 ',\n",
              " '2045 05 01 ',\n",
              " '1840 05 02 ',\n",
              " '1699 05 13 ',\n",
              " '1697 09 18 ',\n",
              " '1608 04 10 ',\n",
              " '1555 06 15 ',\n",
              " '1623 08 12 ',\n",
              " '1698 10 16 ',\n",
              " '2070 05 20 ',\n",
              " '1672 01 21 ',\n",
              " '1702 03 19 ',\n",
              " '1838 11 01 ',\n",
              " '1615 08 29 ',\n",
              " '2061 12 20 ',\n",
              " '2050 03 14 ',\n",
              " '1810 09 03 ',\n",
              " '1836 09 05 ',\n",
              " '1701 05 22 ',\n",
              " '2014 12 24 ',\n",
              " '1906 06 16 ',\n",
              " '1600 02 03 ',\n",
              " '1767 04 06 ',\n",
              " '1978 09 29 ',\n",
              " '1548 03 11 ',\n",
              " '2032 09 13 ',\n",
              " '1943 12 30 ',\n",
              " '1998 01 25 ',\n",
              " '1920 04 03 ',\n",
              " '1841 02 22 ',\n",
              " '1959 07 14 ',\n",
              " '1800 11 25 ',\n",
              " '1938 10 09 ',\n",
              " '2001 07 25 ',\n",
              " '1701 11 28 ',\n",
              " '1911 09 03 ',\n",
              " '1790 01 16 ',\n",
              " '1836 07 26 ',\n",
              " '1975 10 20 ',\n",
              " '1988 09 03 ',\n",
              " '1931 07 11 ',\n",
              " '1568 07 05 ',\n",
              " '1626 11 09 ',\n",
              " '1562 07 21 ',\n",
              " '1527 11 28 ',\n",
              " '1995 11 10 ',\n",
              " '1764 05 24 ',\n",
              " '1546 10 02 ',\n",
              " '2052 01 23 ',\n",
              " '2020 08 20 ',\n",
              " '1985 02 25 ',\n",
              " '1928 07 20 ',\n",
              " '1863 02 03 ',\n",
              " '1681 02 25 ',\n",
              " '1629 02 09 ',\n",
              " '1808 03 10 ',\n",
              " '1792 06 09 ',\n",
              " '1794 09 08 ',\n",
              " '2012 11 17 ',\n",
              " '2042 01 31 ',\n",
              " '1829 06 14 ',\n",
              " '1564 06 21 ',\n",
              " '1521 11 20 ',\n",
              " '1623 11 07 ',\n",
              " '1905 09 03 ',\n",
              " '1857 11 25 ',\n",
              " '1858 02 15 ',\n",
              " '1770 02 15 ',\n",
              " '2035 11 08 ',\n",
              " '2031 12 14 ',\n",
              " '1598 05 26 ',\n",
              " '1733 12 28 ',\n",
              " '1526 06 22 ',\n",
              " '2064 11 25 ',\n",
              " '1685 01 24 ',\n",
              " '1838 03 28 ',\n",
              " '1710 08 14 ',\n",
              " '1689 04 16 ',\n",
              " '1696 06 09 ',\n",
              " '1667 08 01 ',\n",
              " '1963 07 30 ',\n",
              " '1861 05 26 ',\n",
              " '1727 04 26 ',\n",
              " '1665 07 21 ',\n",
              " '1774 02 07 ',\n",
              " '1807 08 08 ',\n",
              " '1725 08 17 ',\n",
              " '1793 04 28 ',\n",
              " '1637 05 04 ',\n",
              " '1767 12 27 ',\n",
              " '1987 06 10 ',\n",
              " '1778 06 23 ',\n",
              " '2035 07 20 ',\n",
              " '1660 06 16 ',\n",
              " '2014 07 29 ',\n",
              " '1961 03 17 ',\n",
              " '1990 08 25 ',\n",
              " '1950 09 07 ',\n",
              " '1534 03 02 ',\n",
              " '1628 01 21 ',\n",
              " '1567 04 15 ',\n",
              " '1712 01 02 ',\n",
              " '1994 11 01 ',\n",
              " '1703 01 09 ',\n",
              " '1697 11 14 ',\n",
              " '1813 08 14 ',\n",
              " '1539 01 26 ',\n",
              " '1669 10 18 ',\n",
              " '2032 02 27 ',\n",
              " '1745 05 25 ',\n",
              " '1872 06 18 ',\n",
              " '2054 11 03 ',\n",
              " '2059 11 07 ',\n",
              " '1672 08 20 ',\n",
              " '1671 02 02 ',\n",
              " '1726 03 21 ',\n",
              " '1794 07 13 ',\n",
              " '1929 02 27 ',\n",
              " '1742 01 25 ',\n",
              " '1553 11 02 ',\n",
              " '2026 04 10 ',\n",
              " '1967 06 16 ',\n",
              " '1640 07 16 ',\n",
              " '1877 01 23 ',\n",
              " '1654 07 13 ',\n",
              " '1607 09 17 ',\n",
              " '1897 06 14 ',\n",
              " '1646 05 12 ',\n",
              " '1983 05 10 ',\n",
              " '1712 05 13 ',\n",
              " '1558 04 09 ',\n",
              " '1560 10 13 ',\n",
              " '1792 02 08 ',\n",
              " '1624 10 05 ',\n",
              " '1659 05 07 ',\n",
              " '1664 08 08 ',\n",
              " '1930 12 31 ',\n",
              " '1608 09 20 ',\n",
              " '1935 10 15 ',\n",
              " '1877 04 30 ',\n",
              " '1692 11 18 ',\n",
              " '1613 11 30 ',\n",
              " '1905 04 23 ',\n",
              " '1718 09 26 ',\n",
              " '1968 02 16 ',\n",
              " '1876 10 18 ',\n",
              " '1781 07 15 ',\n",
              " '1758 02 22 ',\n",
              " '1988 07 12 ',\n",
              " '1605 09 11 ',\n",
              " '1524 08 02 ',\n",
              " '1673 03 11 ',\n",
              " '1566 02 04 ',\n",
              " '1963 03 18 ',\n",
              " '1558 02 02 ',\n",
              " '1839 03 25 ',\n",
              " '2016 05 06 ',\n",
              " '1755 06 22 ',\n",
              " '1927 07 31 ',\n",
              " '1994 04 30 ',\n",
              " '1554 05 25 ',\n",
              " '1715 11 04 ',\n",
              " '1999 08 08 ',\n",
              " '1985 08 18 ',\n",
              " '1797 06 25 ',\n",
              " '1894 05 07 ',\n",
              " '1798 12 11 ',\n",
              " '1950 09 15 ',\n",
              " '1909 07 23 ',\n",
              " '1552 02 16 ',\n",
              " '1825 10 20 ',\n",
              " '1968 12 31 ',\n",
              " '1578 11 09 ',\n",
              " '1788 10 04 ',\n",
              " '1666 07 11 ',\n",
              " '1724 05 29 ',\n",
              " '1527 07 27 ',\n",
              " '1956 03 22 ',\n",
              " '1627 06 27 ',\n",
              " '1746 07 07 ',\n",
              " '1529 08 19 ',\n",
              " '1861 12 17 ',\n",
              " '1766 12 10 ',\n",
              " '1626 07 22 ',\n",
              " '1568 02 27 ',\n",
              " '1716 04 17 ',\n",
              " '1789 12 14 ',\n",
              " '1954 03 17 ',\n",
              " '1677 11 30 ',\n",
              " '1804 01 10 ',\n",
              " '1823 12 21 ',\n",
              " '1688 05 27 ',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check if GPU is available\n",
        "if tf.test.gpu_device_name():\n",
        "    print('GPU is available')\n",
        "else:\n",
        "    print('GPU is NOT available')\n",
        "\n",
        "# Assuming you've defined the evaluate function as mentioned earlier\n",
        "\n",
        "result = []  # Initialize a list to store the results\n",
        "\n",
        "# Assuming X_test is your input data\n",
        "for i in range(len(X_test)):\n",
        "    print(i)\n",
        "\n",
        "    # Convert your input to a TensorFlow tensor and move it to the GPU if available\n",
        "    input_tensor = tf.convert_to_tensor(X_test[[i]], dtype=tf.float32)\n",
        "    if tf.test.is_gpu_available():\n",
        "        input_tensor = input_tensor.gpu()\n",
        "\n",
        "    out, sentence = evaluate(input_tensor)\n",
        "    result.append(out)\n"
      ],
      "metadata": {
        "id": "YuQ40Ovt65zZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[0]"
      ],
      "metadata": {
        "id": "1uABanwO8uJ8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "5f54b0aa-144f-4bc8-b709-8df625c5f414"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1710 09 15 '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy.engine.result import ResultInternal\n",
        "# Define two arrays of strings\n",
        "array1 = y_act\n",
        "array2 = result\n",
        "\n",
        "# Initialize a variable to keep track of matching elements\n",
        "matching_count = 0\n",
        "\n",
        "# Iterate through the elements of both arrays\n",
        "for i in range(len(array1)):\n",
        "    # Compare the elements and check for a match\n",
        "    if array1[i] == array2[i]:\n",
        "        matching_count += 1\n",
        "\n",
        "# Calculate accuracy as the ratio of matching elements to the total number of elements\n",
        "accuracy = (matching_count / len(array1)) * 100\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0KeTJyCCj-H",
        "outputId": "3f86330b-9ccf-470e-aacc-7f3ef544fa2b"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 95.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "error=100-accuracy\n",
        "print(error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEre9lQWD74V",
        "outputId": "92fae2ba-670d-4f40-d446-11c7b4a36293"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.074999999999989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos=[]\n",
        "# Initialize a variable to keep track of matching elements\n",
        "for k in range(10):\n",
        "  matching_count = 0\n",
        "\n",
        "  # Iterate through the elements of both arrays\n",
        "  for i in range(len(array1)):\n",
        "      # Compare the elements and check for a match\n",
        "      if array1[i][k] == array2[i][k]:\n",
        "          matching_count += 1\n",
        "\n",
        "  # Calculate accuracy as the ratio of matching elements to the total number of elements\n",
        "  pos.append(matching_count)\n",
        "\n",
        "# Print the accuracy\n",
        "print(pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMhu3zSYEzj-",
        "outputId": "bdb75833-addc-44c1-c429-709778fd2bcf"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7912, 7731, 7907, 7868, 8000, 7989, 7981, 8000, 7986, 7979]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MO8zf3FA4c3S"
      },
      "execution_count": 68,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}